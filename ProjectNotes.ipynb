{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Street Map Case Study\n",
    "## by Marta Alonso\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1. Area of study\n",
    "\n",
    "City of Vigo (Pontevedra) - Spain -> https://www.openstreetmap.org/relation/341381\n",
    "\n",
    "This is my hometown, so, I want to explore what interesting information OSM has about my city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download and analyze the dataset\n",
    "Data is downloaded with the script below using the [Overpass API](http://overpass-api.de/query_form.html):\n",
    "````\n",
    "[out:xml];\n",
    "(\n",
    "  node(42.1125, -8.9597, 42.2847, -8.5855);\n",
    "  <;\n",
    ");\n",
    "out meta;\n",
    "````\n",
    "\n",
    "The datafile size is 59.9MB, let´s have a look at how many elements, nodes and ways are contained in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 269364, 'total_elements': 766760, 'way': 29258}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "#number of nodes and ways\n",
    "def count_fixme_points(filename):\n",
    "    result = {\"node\": 0,\n",
    "              \"way\": 0,\n",
    "              \"total_elements\" : 0\n",
    "           }\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        result[\"total_elements\"] += 1\n",
    "        if elem.tag == \"node\":\n",
    "            result[\"node\"] += 1\n",
    "        elif elem.tag == \"way\":\n",
    "            result[\"way\"] += 1\n",
    "    return result\n",
    "\n",
    "result = count_fixme_points(\"vigo_box.osm\")\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 766760 elements with 269364 nodes and 29258 ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Auditing data\n",
    "#### Is the data accurate?\n",
    "We only want to take into account data that is reliable, hence we want to check all the fixme tags, and see what comments they include. (The code from this chapter is included in the **audit_fixme.py** script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {'*': 7,\n",
      "             'averiguar nome': 1,\n",
      "             'codigo=o99, otras actividades. afinar office=x si es posible.': 7,\n",
      "             'comprobar que es correcto': 131,\n",
      "             'comprobar que es correcto el etiquetado': 1,\n",
      "             'comprobar si es frutas beni o frutería beni': 1,\n",
      "             'comprobar si es sea parking público o al aire libre. en caso de serlo debería ser amenity= parking.': 1,\n",
      "             'comprobar sobre terreno': 4,\n",
      "             'la mitad hacia el oeste puede ser el 44 de avenida de galicia': 2,\n",
      "             'maxheight': 3,\n",
      "             'name': 1,\n",
      "             'name,operator': 1,\n",
      "             'possible wrong housenumber': 1,\n",
      "             \"what's that?\": 1})\n"
     ]
    }
   ],
   "source": [
    "# Code to create a dictionary with the different \"fixme\" \n",
    "# comments and the number of appearances differente comments have.\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "def fixme_points(filename):\n",
    "    dict = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib[\"k\"] == \"fixme\"):\n",
    "                    dict[tag.attrib[\"v\"].lower().strip()] += 1\n",
    "    return dict\n",
    "\n",
    "cities = fixme_points('vigo_box.osm')\n",
    "pprint.pprint(cities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these \"fixme´d\" data points say \"check for correctness\" or other comments meaning that the data could be inaccurate. Therefore, we will drop these data points, we need a function that solves whether data points need to be fixed/double checked or not, the function would be the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_reliable(elem):\n",
    "    for tag in elem.iter(\"tag\"):\n",
    "        if (tag.attrib[\"k\"] != \"fixme\"):\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of the points are reliable and how many are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixme': 162, 'reliable': 298460}\n"
     ]
    }
   ],
   "source": [
    "def count_fixme_points(filename):\n",
    "    result = {\"fixme\": 0,\n",
    "            \"reliable\": 0\n",
    "           }\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            found = False\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if (tag.attrib[\"k\"] == \"fixme\"):\n",
    "                    found = True\n",
    "            if found:\n",
    "                result[\"fixme\"] += 1\n",
    "            else:\n",
    "                result[\"reliable\"] += 1\n",
    "    return result\n",
    "\n",
    "result = count_fixme_points(\"vigo_box.osm\")\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we have a total of 298460 reliable points and 162 points that need to be double checked. We will drop those 162."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where is the data from?\n",
    "\n",
    "We want to use only data from the city of Vigo, so we need to audit the city names of the data points. We will use the function below (the code for this chapter is included in the **audit_cities.py** script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {'BAIONA': 5,\n",
      "             'Baiona': 17,\n",
      "             'Cangas': 85,\n",
      "             'Cans': 1,\n",
      "             'Gondomar': 6,\n",
      "             'Moaña': 5,\n",
      "             'Mos': 15,\n",
      "             'Nigran': 6,\n",
      "             'Nigrán': 5,\n",
      "             'Nigrán Priegue, Pontevedra': 1,\n",
      "             'O Cruceiro - Cedeira - Redondela': 1,\n",
      "             'O Porriño': 132,\n",
      "             'Porriño (O)': 1,\n",
      "             'Redondela': 16,\n",
      "             'Sanguiñeda': 1,\n",
      "             'Sanguiñeda - Mos': 1,\n",
      "             'Valladares Vigo': 1,\n",
      "             'Vigo': 964,\n",
      "             'cangas': 2,\n",
      "             'vigo': 9})\n"
     ]
    }
   ],
   "source": [
    "# Code to determine which city the data points belong to\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "def city_names(filename):\n",
    "    dict = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            if is_reliable(elem):\n",
    "                for tag in elem.iter(\"tag\"):\n",
    "                    if (tag.attrib[\"k\"] == \"addr:city\"):\n",
    "                        dict[tag.attrib[\"v\"]] += 1\n",
    "    return dict\n",
    "\n",
    "cities = city_names('vigo_box.osm')\n",
    "pprint.pprint(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have discovered two problems here, since we are using a box to download Vigo´s data, we are getting also many nodes from other cities, but not only that, we have upper and lower caps for Vigo in several nodes, we need to take care of this. \n",
    "\n",
    "We create a function to check if the node belongs to Vigo and to convert the city names to lowercase to homogenize those values. Apart from the \"addr_city\" attribute, we include in the search the \"is_in:city\" and \"is_in:municipality\" since we have realized any of them could specify which city the element belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if a xml element \n",
    "# belongs to a specified city or not\n",
    "\n",
    "def is_my_city(elem, city):\n",
    "    for tag in elem.iter(\"tag\"):\n",
    "        if ((tag.attrib[\"k\"] == \"addr:city\") or\n",
    "            (tag.attrib[\"k\"] == \"is_in:city\") or\n",
    "            (tag.attrib[\"k\"] == \"is_in:municipality\")):\n",
    "            if (city == tag.attrib[\"v\"].lower()):\n",
    "                return True\n",
    "    return False         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are street names homogeneous?\n",
    "We want to audit the accuracy and uniformity of the street names. All of them should include any of the expected street types in Galician language (rúa, avenida, camiño, etc). We use the following code to check unexpected street types in our dataset. The code for this chapter is included in the script **audit_streets.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1ª': {'1ª Travesía do Couto de San Honorato'},\n",
      " 'AVDA': {'AVDA DE MADRID',\n",
      "          'AVDA. MADRID',\n",
      "          'AVDA. RICARDO MELLA, S/N (PO-325 KM 5.8)'},\n",
      " 'AVENIDA': {'AVENIDA BEIRAMAR',\n",
      "             'AVENIDA GALICIA-TEIS',\n",
      "             'AVENIDA MADRID, S/N',\n",
      "             'AVENIDA RICARDO MELLA, S/N'},\n",
      " 'Alcalde': {'Alcalde Gregorio Espino'},\n",
      " 'Aragon': {'Aragon'},\n",
      " 'Aragón': {'Aragón'},\n",
      " 'As': {'As Lagoas, Marcosende'},\n",
      " 'Atlantida': {'Atlantida'},\n",
      " 'Baixada': {'Baixada a Rúa Conde de Torrecedeira'},\n",
      " 'Balaidos': {'Balaidos'},\n",
      " 'Bao': {'Bao'},\n",
      " 'Bembrive': {'Bembrive'},\n",
      " 'Buenos': {'Buenos Aires'},\n",
      " 'C/Alcalde': {'C/Alcalde de Lavadores'},\n",
      " 'CALLE': {'CALLE TOMAS PAREDES'},\n",
      " 'CARRETERA': {'CARRETERA MADRID - VIGO KM 663.5',\n",
      "               'CARRETERA VIGO- MADRID KM. 663'},\n",
      " 'CL': {'CL AVENIDA DE MADRID',\n",
      "        'CL REPUBLICA ARGENTINA 8',\n",
      "        'CL TRAVESIA DE VIGO'},\n",
      " 'Calle': {'Calle C'},\n",
      " 'Cambados': {'Cambados'},\n",
      " 'Camelias': {'Camelias'},\n",
      " 'Camino': {'Camino da Seara', 'Camino da Galindra', 'Camino Camilo Tuche'},\n",
      " 'Camposancos': {'Camposancos'},\n",
      " 'Campus': {'Campus de Vigo, Lagoas-Marcosende'},\n",
      " 'Canovas': {'Canovas Del Castillo'},\n",
      " 'Castelao': {'Castelao'},\n",
      " 'Castrelos': {'Castrelos'},\n",
      " 'Cesareo': {'Cesareo Vázquez'},\n",
      " 'Costa': {'Costa'},\n",
      " 'Couto': {'Couto Piñeiro'},\n",
      " 'Cronista': {'Cronista Rodríguez Elias'},\n",
      " 'E': {'E. Mtnez Garrido-Alcalde'},\n",
      " 'Ecuador': {'Ecuador'},\n",
      " 'Enrique': {'Enrique Lorenzo'},\n",
      " 'Estorniño': {'Estorniño'},\n",
      " 'Eugenio': {'Eugenio Fadrique'},\n",
      " 'Faisan': {'Faisan'},\n",
      " 'Fonte': {'Fonte das Abelleiras, Campus Universitario de Vigo'},\n",
      " 'Galicia': {'Galicia'},\n",
      " 'García': {'García Barbon'},\n",
      " 'Girona': {'Girona'},\n",
      " 'Gonzalez': {'Gonzalez Sierra'},\n",
      " 'Gran': {'Gran Via'},\n",
      " 'Grove': {'Grove (O)'},\n",
      " 'Hispanidade': {'Hispanidade'},\n",
      " 'Illa': {'Illa de Toralla,  Coruxo'},\n",
      " 'Illas': {'Illas Baleares', 'Illas Canarias'},\n",
      " 'Jaime': {'Jaime Balmes'},\n",
      " 'Jenaro': {'Jenaro De La Fuente'},\n",
      " 'Jose': {'Jose Gómez Posada-Curros'},\n",
      " 'Lagoas': {'Lagoas Marcosende (Campus Universitario)'},\n",
      " 'Laxe': {'Laxe'},\n",
      " 'Leonardo': {'Leonardo Alonso'},\n",
      " 'Macal': {'Macal'},\n",
      " 'Manuel': {'Manuel De Castro'},\n",
      " 'Marcosende': {'Marcosende'},\n",
      " 'Meixoeiro': {'Meixoeiro'},\n",
      " 'Miradoiro': {'Miradoiro (Do)'},\n",
      " 'Paseo': {'Paseo marítimo da ETEA'},\n",
      " 'Pescadores': {'Pescadores'},\n",
      " 'Pizarro': {'Pizarro'},\n",
      " 'Playa': {'Playa de Canido'},\n",
      " 'Plaza': {'Plaza de la Estrella '},\n",
      " 'Praza/patio': {'Praza/patio interior'},\n",
      " 'Principe': {'Principe'},\n",
      " 'Progreso': {'Progreso'},\n",
      " 'Provincial': {'Provincial'},\n",
      " 'Ramon': {'Ramon Nieto'},\n",
      " 'Ramón': {'Ramón del Valle-Inclán'},\n",
      " 'Republica': {'Republica Argentina'},\n",
      " 'Ricardo': {'Ricardo Mella'},\n",
      " 'Ronda': {'Ronda Don Bosco'},\n",
      " 'Rosalía': {'Rosalía de castro', 'Rosalía de Castro, 28'},\n",
      " 'Rotonda': {'Rotonda Zona Franca de Bouzas'},\n",
      " 'Rua': {'Rua Abeleira Menendez',\n",
      "         'Rua Leonardo Da Vinci',\n",
      "         'Rua Ramon Nieto',\n",
      "         'Rua Salceda de Caselas',\n",
      "         'Rua Salgado',\n",
      "         'Rua Teofilo Llorente',\n",
      "         \"Rua de Alfonso X 'O Sabio'\"},\n",
      " 'Salgueira': {'Salgueira'},\n",
      " 'Salvaterra': {'Salvaterra'},\n",
      " 'San': {'San Cristovo', 'San Roque'},\n",
      " 'Sanjurjo': {'Sanjurjo Badía', 'Sanjurjo Badia'},\n",
      " 'Santa': {'Santa Marina'},\n",
      " 'Senda': {'Senda do río Eifonso'},\n",
      " 'Senra': {'Senra De Abaixo'},\n",
      " 'Subida': {'Subida Relfas'},\n",
      " 'TRAVESIA': {'TRAVESIA DE VIGO'},\n",
      " 'Teixugueiras': {'Teixugueiras (As)'},\n",
      " 'Toledo': {'Toledo'},\n",
      " 'Tomás': {'Tomás A Alonso', 'Tomás Paredes'},\n",
      " 'Travesia': {'Travesia De Vigo', 'Travesia de Jacinto Benavente'},\n",
      " 'Travesía': {'Travesía de Vigo', 'Travesía do Príncipe'},\n",
      " 'Urzaiz': {'Urzaiz'},\n",
      " 'Valadares': {'Valadares'},\n",
      " 'Venezuela': {'Venezuela'},\n",
      " 'Via': {'Via Norte'},\n",
      " 'Zamora': {'Zamora'}}\n"
     ]
    }
   ],
   "source": [
    "# Code to determine which street types are included in the dataset\n",
    "# others than the expected ones in the \"expected\" array.\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"vigo_box.osm\"\n",
    "street_type_re = re.compile(r'^\\S+\\.?\\b', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Rúa\", \"Avenida\", \"Camiño\", \"Praza\", \"Lugar\", \"Estrada\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            if (is_reliable(elem) and is_my_city(elem, \"vigo\")):\n",
    "                for tag in elem.iter(\"tag\"):\n",
    "                    if is_street_name(tag):\n",
    "                        audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "st_types = audit(OSMFILE)\n",
    "pprint.pprint(dict(st_types))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the street types and names are far from homogenous, there are several problems with them: \n",
    "\n",
    "* written inconsistencies: some street names are written both with accent marks and without them, \n",
    "* language inconsistencies: there are street types like \"street\" or \"square\" written both in Spanish and in Galician (regional language) so the same street could appear twice written differently (calle/rua, plaza/praza)\n",
    "* case inconsistencies: the same street is written some times in upper and sometimes in lower cases\n",
    "* names like street(calle/rua) are sometimes abbreviated (calle, cl, c/)\n",
    "\n",
    "To solve these issues we need the following functions that are included in the script **audit_streets.py**. As shown below the list of expected street types was extended and homogenized in lower caps and without accent marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean and homogenize the street names and types\n",
    "\n",
    "expected = [\"rua\", \"avenida\", \"camiño\", \"praza\", \"lugar\", \"estrada\", \"caleixon\", \"poligono\", \"area\", \"paseo\"]\n",
    "\n",
    "mapping = { \"avda\": \"avenida\",\n",
    "            \"calle\": \"rua\",\n",
    "            \"cl\" : \"rua\",\n",
    "            \"carretera\" : \"estrada\",\n",
    "            \"ctra\" : \"estrada\",\n",
    "            \"calleja/callejón\" : \"caleixon\",\n",
    "            \"plaza\" : \"praza\",\n",
    "            \"praza/patio\" : \"praza/patio\",\n",
    "            \"C/Alcalde\" : \"rua alcalde\"\n",
    "           }\n",
    "\n",
    "def delete_accents(word):\n",
    "    return word.replace(u\"á\", u\"a\").replace(u\"é\", u\"e\").replace(u\"í\", u\"i\").replace(u\"ó\", u\"o\").replace(u\"ú\", u\"u\")\n",
    "    #return word\n",
    "\n",
    "def get_street(elem):\n",
    "    city = None\n",
    "    for tag in elem.iter(\"tag\"):\n",
    "        if (tag.attrib[\"k\"] == \"addr:street\"):\n",
    "            street = delete_accents(tag.attrib[\"v\"].lower().strip())\n",
    "            m = street_type_re.search(street)\n",
    "            if m:\n",
    "                street_type = m.group()\n",
    "            if street_type in expected:\n",
    "                return street\n",
    "            elif street_type in mapping.keys():\n",
    "                street = street.replace(street_type, mapping[street_type])\n",
    "                return street\n",
    "            else:\n",
    "                return \"rua \"+street\n",
    "    return city   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning data\n",
    "We are going to read the data from the osm file, clean it with the different functions we have created in the previous chapters, and transform it to a json file to be imported into MongoDB. The format of the JSON is going to be similar to the one used in the course case of study. Following the same rules:\n",
    "- only 2 types of top level tags will be processed: \"node\" and \"way\"\n",
    "- all attributes of \"node\" and \"way\" will be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array that will be added under a key \"created\"\n",
    "    - attributes for latitude and longitude that will be added to a \"pos\" array. \n",
    "- if the second level tag \"k\" value contains problematic characters, it will be ignored\n",
    "- if the second level tag \"k\" value starts with \"addr:\", it will be added to a dictionary \"address\"\n",
    "- if the second level tag \"k\" value does not start with \"addr:\", but contains \":\", the colon will be converted to a valid character like \"_\".\n",
    "- if there is a second \":\" that separates the type/direction of a street, the tag will be ignored\n",
    "- for \"way\" specifically:\n",
    "```\n",
    "  <nd ref=\"305896090\"/>\n",
    "  <nd ref=\"1719825889\"/>\n",
    "```\n",
    "will be turned into\n",
    "```\n",
    "\"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "```\n",
    "An example of the new JSON shape is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type\": \"node\",\n",
    "\"visible\":\"true\",\n",
    "\"created\": {\n",
    "          \"version\":\"2\",\n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "          \"user\":\"linuxUser16\",\n",
    "          \"uid\":\"1219059\"\n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"60625\",\n",
    "          \"street\": \"North Lincoln Ave\"\n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"1 (773)-271-5176\"\n",
    "}\n",
    "```\n",
    "\n",
    "The code shown below can be found at **data_process.py** script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to process the xml file, cleanint it a transforming into JSON\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "#from audit_fixme import is_reliable\n",
    "#from audit_cities import is_my_city\n",
    "#from audit_streets import get_street\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        if is_reliable(element) and is_my_city(element, \"vigo\"):\n",
    "            node['id'] = element.attrib.get('id') \n",
    "            node['type'] = element.tag \n",
    "            node['visible'] = element.attrib.get('visible') \n",
    "            node['created'] = {}\n",
    "            node['created']['version'] = element.attrib.get('version')\n",
    "            node['created']['changeset'] = element.attrib.get('changeset')\n",
    "            node['created']['timestamp'] = element.attrib.get('timestamp')\n",
    "            node['created']['user'] = element.attrib.get('user')\n",
    "            node['created']['uid'] = element.attrib.get('uid')\n",
    "            if element.tag == \"node\" :\n",
    "                node['pos'] = [float(element.attrib.get('lat')), float(element.attrib.get('lon'))]\n",
    "            if element.tag == \"way\" :\n",
    "                node['node_refs'] = []\n",
    "                for nd in element.iter('nd'):\n",
    "                    node['node_refs'].append(nd.attrib['ref'])\n",
    "            for tag in element.iter('tag'):\n",
    "                if problemchars.search(tag.attrib['k']):\n",
    "                    pass\n",
    "                elif lower.search(tag.attrib['k']):\n",
    "                    node[tag.attrib['k']] = tag.attrib['v']\n",
    "                else:\n",
    "                    m = lower_colon.search(tag.attrib['k'])\n",
    "                    if m:\n",
    "                        if len(m.group().split(\":\")) != 2 :\n",
    "                            pass\n",
    "                        elif m.group().split(\":\")[0] == \"addr\":\n",
    "                            if \"address\" not in node.keys():\n",
    "                                node['address'] = {}\n",
    "                            if m.group().split(\":\")[1] == \"street\":\n",
    "                                node['address']['street'] = get_street(element)\n",
    "                            else:\n",
    "                                node['address'][m.group().split(\":\")[1]] = tag.attrib['v']\n",
    "                        else:\n",
    "                            new = tag.attrib['k'].replace(\":\", \"_\")\n",
    "                            node[new] = tag.attrib['v']\n",
    "                    \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "data = process_map('vigo_box.osm', False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Working with MongoDB\n",
    "\n",
    "#### Importing into DB\n",
    "The json file we have just created is importend into MongoDB with the following command:\n",
    "````\n",
    "mongoimport -d osm_db -c vigo_col --file vigo_box.osm.json\n",
    "````\n",
    "\n",
    "#### Using console to query the DB\n",
    "We connect to the MongoDB console and launch several queries\n",
    "```\n",
    "$Project> mongo\n",
    "MongoDB shell version: 3.0.15\n",
    "connecting to: test\n",
    "> use osm_db\n",
    "switched to db osm_db\n",
    "```\n",
    "\n",
    "Let´s count the number of nodes and ways in the DB\n",
    "\n",
    "```\n",
    "> db.vigo_col.find({\"type\":\"node\"}).count()\n",
    "580\n",
    "> db.vigo_col.find({\"type\":\"way\"}).count()\n",
    "439\n",
    "```\n",
    "We have 580 nodes and 439 ways in our DB.\n",
    "```\n",
    "> db.vigo_col.distinct(\"created.uid\").length\n",
    "147\n",
    "```\n",
    "There are 147 unique users who have contributed to that data from Vigo city.\n",
    "\n",
    "Let´s do some aggregation queries:\n",
    "```\n",
    "> db.vigo_col.aggregate([{\"$group\" : {\"_id\" : \"$amenity\", \"count\": {\"$sum\" : 1}}},{\"$sort\" : {\"count\" : -1}}, {\"$limit\" : 10}])\n",
    "{ \"_id\" : null, \"count\" : 678 }\n",
    "{ \"_id\" : \"pharmacy\", \"count\" : 112 }\n",
    "{ \"_id\" : \"cafe\", \"count\" : 36 }\n",
    "{ \"_id\" : \"restaurant\", \"count\" : 33 }\n",
    "{ \"_id\" : \"bank\", \"count\" : 30 }\n",
    "{ \"_id\" : \"bar\", \"count\" : 23 }\n",
    "{ \"_id\" : \"school\", \"count\" : 22 }\n",
    "{ \"_id\" : \"fuel\", \"count\" : 16 }\n",
    "{ \"_id\" : \"fast_food\", \"count\" : 8 }\n",
    "{ \"_id\" : \"place_of_worship\", \"count\" : 6 }\n",
    "```\n",
    "We have a list of the 10 most common amenities, been the first one in the list the number of elements without an amenity tag. We can see that pharmacies are the most common one and cafes, restaurants and banks are the second most common amenities in the dataset.\n",
    "```\n",
    "> db.vigo_col.aggregate([{\"$match\" : {\"amenity\" : \"pharmacy\"}},{\"$group\" : {\"_id\" : \"$address.street\", \"count\": {\"$sum\" : 1}}},{\"$sort\" : {\"count\" : -1}}, {\"$limit\" : 5}])\n",
    "{ \"_id\" : \"rua ramon nieto\", \"count\" : 6 }\n",
    "{ \"_id\" : \"rua travesia de vigo\", \"count\" : 6 }\n",
    "{ \"_id\" : \"rua urzaiz\", \"count\" : 6 }\n",
    "{ \"_id\" : \"rua sanjurjo badia\", \"count\" : 5 }\n",
    "{ \"_id\" : \"rua teixugueiras (as)\", \"count\" : 3 }\n",
    "\n",
    "> db.vigo_col.aggregate([{\"$match\" : {\"amenity\" : \"cafe\"}},{\"$group\" : {\"_id\" : \"$address.street\", \"count\": {\"$sum\" : 1}}},{\"$sort\" : {\"count\" : -1}}, {\"$limit\" : 5}])\n",
    "{ \"_id\" : \"avenida da florida\", \"count\" : 6 }\n",
    "{ \"_id\" : \"rua travesia de vigo\", \"count\" : 4 }\n",
    "{ \"_id\" : \"rua de venezuela\", \"count\" : 3 }\n",
    "{ \"_id\" : \"rua puerto rico\", \"count\" : 2 }\n",
    "{ \"_id\" : \"avenida da gran via\", \"count\" : 2 }\n",
    "```\n",
    "With the last two queries we wanted to find the top 5 streets with the biggest numbers of pharmacies and cafes. In the case of pharmacies that results make a lot of sense since those streets are some of the longest streets in Vigo. For the cafe´s query, the streets in the results are long streets situated in the city centre which also make a lot of sense. Except for the case of the Street Puerto Rico which is in the city center but it is much smaller than the other streets in the top 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Future improvements\n",
    "This was just an example of what can be done cleaning and auditing a dataset, but we could go further, auditing and cleaning other fields in the dataset. \n",
    "For example we could audit a cross field constraint like cities and postal codes. \n",
    "Let´s make some exploring queries about postal codes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> db.vigo_col.find({\"address.postcode\":{\"$exists\":1}}).count()\n",
    "897\n",
    "> db.vigo_col.find({\"address.postcode\":{\"$exists\":0}}).count()\n",
    "122\n",
    "```\n",
    "We can see that from the 1019 elements in our filtered dataset, 897 have the tag \"address.postcode\" and 122 don´t. \n",
    "Let´s try to match now the postal codes of Vigo with the ones in the dataset:\n",
    "\n",
    "```\n",
    "> db.vigo_col.find({\"address.postcode\":{\"$in\":[\"36201\", \"36202\", \"36203\", \"36204\", \"36205\", \"36206\", \"36207\", \"36208\", \"36209\", \"36210\", \"36211\", \"36212\", \"36213\", \"36214\", \"36215\", \"36216\", \"36312\", \"36317\", \"36331\"]}}).count()\n",
    "871\n",
    "```\n",
    "From those 897 with postal code, 871 are postalcodes that indeed match the existing postal codes in Vigo. What happen with the remaining 26?\n",
    "```\n",
    "> db.vigo_col2.find({\"address.postcode\":{\"$nin\":[\"36201\", \"36202\", \"36203\", \"36204\", \"36205\", \"36206\", \"36207\", \"36208\", \"36209\", \"36210\", \"36211\", \"36212\", \"36213\", \"36214\", \"36215\", \"36216\", \"36312\", \"36317\", \"36331\"],\"$exists\":1}},{\"_id\":0,\"address.postcode\":1})\n",
    "{ \"address\" : { \"postcode\" : \"36314\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36314\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36315\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36025\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36200\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36390\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36314\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36330\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36310\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36314\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36390\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36318\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36310\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36310\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36315\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36315\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36310\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36318\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36310\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36200\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36314\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36330\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36330\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36392\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36310\" } }\n",
    "{ \"address\" : { \"postcode\" : \"36350\" } }\n",
    "```\n",
    "These are the 26 postal codes that do not belong to Vigo, some of them, like the 36200 or 36025, are probably manual mistakes (36025 is probably 36205). A further investigation could analyze which cities those postal codes belong, if they really exists in other cities, or whether they are close to Vigo or not. The mistake could be in the postal code itself of in the city that the creator of the element wrote. Analyzing the content of the elements, name of the place and other fields would help to determine which of the mistakes happened in each case.\n",
    "\n",
    "Going a little bit further, the location coordinates could be used to determine where exactly the point is, and which city and postal code it belongs to, to determine where the mistake was made. The problem in this kind of investigation is to choose, having the location, the city and the postal code, the right and the wrong field, we need always more information about the element, like name or description. Also if two out of three fields match that would help us to choose sides here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES\n",
    "\n",
    "* [https://wiki.openstreetmap.org/wiki/OSM_XML](https://wiki.openstreetmap.org/wiki/OSM_XML)\n",
    "* [https://wiki.openstreetmap.org/wiki/Elements](https://wiki.openstreetmap.org/wiki/Elements)\n",
    "https://wiki.openstreetmap.org/wiki/Map_features\n",
    "https://wiki.openstreetmap.org/wiki/Key:is_in\n",
    "http://nbconvert.readthedocs.io/en/5.x/install.html\n",
    "* [https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook](https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook)\n",
    "* [http://data-blog.udacity.com/posts/2016/10/latex-primer/](http://data-blog.udacity.com/posts/2016/10/latex-primer/)\n",
    "* [https://github.com/jupyter/help/issues/163](https://github.com/jupyter/help/issues/163)\n",
    "* [https://docs.mongodb.com/manual/reference/](https://docs.mongodb.com/manual/reference/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py36]",
   "language": "python",
   "name": "conda-env-ipykernel_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
